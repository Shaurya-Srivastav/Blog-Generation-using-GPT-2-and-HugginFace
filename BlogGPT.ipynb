{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ***0. Installing Dependencies***"
      ],
      "metadata": {
        "id": "55Wda6KK3LbC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4V1H_yrR3A6m",
        "outputId": "68df886f-35ed-44b0-91dd-72a5b5535d09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.28.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers #installing transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer \n",
        "##importing Language model\n",
        "#importing tokenizer to create pytorch tensors and tokenize sentences\n"
      ],
      "metadata": {
        "id": "yqWd6NYB4sLm"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***1. Load Model***"
      ],
      "metadata": {
        "id": "IaCvk_Tu3bcg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2-large') #intialized tokenizer using gpt-large \n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2-large\", pad_token_id=tokenizer.eos_token_id)"
      ],
      "metadata": {
        "id": "tH7nDHBI5Io2"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.eos_token_id\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7UTeiKCv6Erk",
        "outputId": "06dc4a78-40e5-40dd-958e-092ebf79da7a"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50256"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***2. Tokenize sentences***"
      ],
      "metadata": {
        "id": "2GIBIEGM3bhJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = input(\"Tell me a sentence: \")\n",
        "maxLength = int(input(\"How many words would you like it to be (whole numbers only): \"))\n",
        "#can input maximum words \n",
        "input_ids = tokenizer.encode(sentence, return_tensors ='pt') #returns pytorch tensors"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9tdHEDEY6MKx",
        "outputId": "8f7beda7-a5e2-45ad-885e-d7e02bf8e44a"
      },
      "execution_count": 39,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tell me a sentence: I love coding\n",
            "How many words would you like it to be (whole numbers only): 200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(input_ids[0][1]) #input ids are sentences stored as a pytorch array\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "lQjmwN5I6zV2",
        "outputId": "a1541b6f-886b-45bc-836e-a59ab4c76f39"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' love'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***3. Generate and Decode Text***"
      ],
      "metadata": {
        "id": "pMjGPUHn30iT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output = model.generate(input_ids, max_length = maxLength, no_repeat_ngram_size = 2, early_stopping = True)\n",
        "#generating output text using the model\n",
        "#takes in sentence tokens(input_ids)\n",
        "#can specify max lenght, repeat, and early_stopping \n",
        "#uses beamsearch with 5 trees\n"
      ],
      "metadata": {
        "id": "YnBpFCjn89JT"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output\n",
        "#returns generated text as a pytorch tensor token to decode"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cebQOl7Z94c9",
        "outputId": "cb4c75b9-ca1d-4e5f-983b-87d63c4781e6"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[   40,  1842, 19617,    11,   475,   314,  5465,  3597,  2438,    13,\n",
              "           314,  1842,  3597,  3788,    11,   290,   314,  1101,   407,   257,\n",
              "         24292,    13,   198,   198,    40,  1101,   257, 11915,    13,   843,\n",
              "           314,   588,   284,  1486,    13,   887,   314,   836,   470,   588,\n",
              "          3597,    13,  1406,   314,  1053,   587,  2111,   284,  3785,   503,\n",
              "           703,   284,  3551,  2438,   326,   314,   460,  1682,   779,    13,\n",
              "           632,   338,   587,   257,  6531,    13,   383,   717,  1517,   314,\n",
              "           750,   373,  3551,   257,  1430,   326,   561,  1309,   502,  3551,\n",
              "           262,  2438,   314,  2227,   284,    13,  3244,   314,  2630,   257,\n",
              "          1218,  1430,   284,  1309,   262,   717,  1430,  1057,    13,  1320,\n",
              "           373,   257,  1256,   286,   670,    13,   357,    40,  1053,   635,\n",
              "          3194,   257,  1178,   584,  4056,   326,  1309,   345,  3551,   534,\n",
              "           898,  2438,  2014,   198,    13,   764,   764,   198,   464,  1218,\n",
              "          1517,   326,  3022,   373,   326,   618,   314,  2067,  3597,   262,\n",
              "          1430,    11,   314,  6939,   326,   340,   373,  1682,   257,   845,\n",
              "          2829,  1430,    13,  1318,   373,   645,  1103,  9156,    13,  1400,\n",
              "          1103,  2438,   284,  1100,    13,  2329,   257,  7684,   286,  3146,\n",
              "            13,   317,  7684,    13,  3226,  3146,   326,   547,   477,   262,\n",
              "           976,  2546,    13,  1439,   262,  3146,   547,   262,  2546,   286,\n",
              "           262, 18197,  1271,   326,   714,   307,  7997,   287,   257,  3644]])"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(output[0], skip_special_tokens = True) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "CAXN-9N--CoR",
        "outputId": "de59ce17-e5c8-4d79-a021-e7dff0e94f70"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I love coding, but I hate writing code. I love writing software, and I'm not a programmer.\\n\\nI'm a designer. And I like to design. But I don't like writing. So I've been trying to figure out how to write code that I can actually use. It's been a struggle. The first thing I did was write a program that would let me write the code I wanted to. Then I wrote a second program to let the first program run. That was a lot of work. (I've also written a few other programs that let you write your own code.)\\n...\\nThe second thing that happened was that when I started writing the program, I realized that it was actually a very simple program. There was no real logic. No real code to read. Just a bunch of numbers. A bunch. Of numbers that were all the same size. All the numbers were the size of the smallest number that could be represented in a computer\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***4. Output result***"
      ],
      "metadata": {
        "id": "0cJUpBth3-SO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = tokenizer.decode(output[0], skip_special_tokens = True) \n",
        "#stores output in text without any special keywords using the skip_special_tokens\n"
      ],
      "metadata": {
        "id": "M_v4yKV1_fgK"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('blogposticecream.txt', 'w') as f:\n",
        "  f.write(text)"
      ],
      "metadata": {
        "id": "BHj5R8OK_onn"
      },
      "execution_count": 50,
      "outputs": []
    }
  ]
}